{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Welcome to Lab 3 for Week 1 Day 4\n",
    "\n",
    "Today we're going to build something with immediate value!\n",
    "\n",
    "In the folder `me` I've put a single file `linkedin.pdf` - it's a PDF download of my LinkedIn profile.\n",
    "\n",
    "Please replace it with yours!\n",
    "\n",
    "I've also made a file called `summary.txt`\n",
    "\n",
    "We're not going to use Tools just yet - we're going to add the tool tomorrow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left; width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../assets/tools.png\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#00bfff;\">Looking up packages</h2>\n",
    "            <span style=\"color:#00bfff;\">In this lab, we're going to use the wonderful Gradio package for building quick UIs, \n",
    "            and we're also going to use the popular PyPDF2 PDF reader. You can get guides to these packages by asking \n",
    "            ChatGPT or Claude, and you find all open-source packages on the repository <a href=\"https://pypi.org\">https://pypi.org</a>.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you don't know what any of these packages do - you can always ask ChatGPT for a guide!\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pypdf import PdfReader\n",
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "MODEL = \"llama3.2:3b\"\n",
    "openai = OpenAI(base_url=\"http://localhost:11434/v1\", api_key=\"ollama\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = PdfReader(\"me/linkedin_profile.pdf\")\n",
    "linkedin = \"\"\n",
    "for page in reader.pages:\n",
    "    text = page.extract_text()\n",
    "    if text:\n",
    "        linkedin += text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   \n",
      "Contact\n",
      "11 No. Roynagar Housing Co-\n",
      "operative Society Kolkata-70\n",
      "8583895431 (Mobile)\n",
      "sportshirsendu@gmail.com\n",
      "www.linkedin.com/in/shirsendu-\n",
      "dhar-470157147 (LinkedIn)\n",
      "Top Skills\n",
      "PyTorch\n",
      "LLM\n",
      "Linear Regression\n",
      "Certifications\n",
      "Sequences, Time Series and\n",
      "Prediction\n",
      "Big Data with Spark and Hadoop\n",
      "Essentials\n",
      "Generative AI with Large Language\n",
      "Models\n",
      "Neural Networks and Deep Learning\n",
      "Publications\n",
      "Feather-Light Vessel Segregation\n",
      "Model\n",
      "eXtreme Gradient Boosting Scheme\n",
      "for Fundus Vessels Separation\n",
      "Shirsendu Dhar\n",
      "Data Scientist @ Conde Nast | Pursuing Postgraduate in Applied\n",
      "Statistics\n",
      "Bengaluru, Karnataka, India\n",
      "Summary\n",
      "At Fractal, my focus as a Data Scientist is on enhancing digital\n",
      "engagement and customer retention for the second-largest retail\n",
      "chain in the USA. Leveraging my expertise in A/B testing and\n",
      "problem-solving, I apply sophisticated data analysis techniques to\n",
      "drive strategic decisions that resonate with our customers' needs.\n",
      "As I pursue my postgraduate degree in Applied Statistics, my\n",
      "commitment to expanding my knowledge base in personalization\n",
      "aligns with the innovative work at Fractal. My objective is to\n",
      "transform large, complex datasets into actionable insights, ensuring\n",
      "that our data-driven approaches lead to meaningful business\n",
      "outcomes.\n",
      "Experience\n",
      "Condé Nast\n",
      "Data Scientist\n",
      "February 2025 - Present (4 months)\n",
      "Bengaluru, Karnataka, India\n",
      "Fractal\n",
      "3 years 4 months\n",
      "Data Scientist - G8\n",
      "April 2024 - January 2025 (10 months)\n",
      "Bengaluru, Karnataka, India\n",
      "• Developed a Customer Churn Identification model for e-commerce customer\n",
      "segments, pinpointing key factors driving churn for targeted retention\n",
      "strategies.\n",
      "• Utilized causal inference techniques to extract actionable insights and guide\n",
      "data-driven decision-making.\n",
      "• Achieved a 5% improvement in retention rates by implementing interventions\n",
      "for high-risk customer groups.\n",
      "  Page 1 of 3   \n",
      "• Increased customer retention and digital engagement through personalized\n",
      "omni-channel strategies.\n",
      "• Analyzed customer transaction gaps and modeled how exponential decay\n",
      "impacts repeat purchase likelihood, optimizing time-sensitive marketing\n",
      "campaigns.\n",
      "Data Scientist - G9\n",
      "April 2023 - April 2024 (1 year 1 month)\n",
      "Bengaluru, Karnataka, India\n",
      "• Fine-tuned open-source LLM models and API keys to meet domain-specific\n",
      "needs.\n",
      "• Designed and implemented end-to-end ML pipelines for text generation\n",
      "tasks, including data preprocessing, model architecture, training, evaluation,\n",
      "and deployment.\n",
      "• Conducted research on state-of-the-art generative models, including BERT,\n",
      "GPT, and T5, for multilingual applications.\n",
      "• Implemented safety-controlled text composition systems, ensuring reliable\n",
      "and culturally sensitive outputs for various regions.\n",
      "• Conducted ad-hoc analyses and submitted reports to stakeholders for a CPG\n",
      "client.\n",
      "• Worked with unorganized data, quickly grasping tasks to provide meaningful\n",
      "insights.\n",
      "• Recognized for diligent work and strong analytical skills.\n",
      "Associate Data Scientist - G10\n",
      "October 2021 - March 2023 (1 year 6 months)\n",
      "Bengaluru, Karnataka, India\n",
      "• Developed a model to classify transactions into recurring and non-recurring,\n",
      "predicting next date & amount.\n",
      "• Utilized past 6 months transaction history to understand recurring patterns\n",
      "and prevent overdraft fees.\n",
      "• Enhanced customer experience by alerting customers of possible overdrafts,\n",
      "improving customer relationships.\n",
      "happymonk.ai\n",
      "Data Science Intern\n",
      "April 2021 - August 2021 (5 months)\n",
      "Bengaluru, Karnataka, India\n",
      "• Built a fire and spark detection model using image augmentation and mask-\n",
      "rcnn for fast training.\n",
      "• Implemented gamma correction and noise to replicate real-world scenarios.\n",
      "  Page 2 of 3   \n",
      "• Contributed to happymonk.ai's innovative projects in Bengaluru, Karnataka,\n",
      "India.\n",
      "NullStack Technologies\n",
      "Student Intern\n",
      "April 2020 - May 2020 (2 months)\n",
      "Kolkata metropolitan area, West Bengal, India\n",
      "• Designed a Web Crawler to track domain rankings daily using web scraping\n",
      "for automated updates.\n",
      "• Implemented innovative solutions to streamline processes and eliminate\n",
      "manual input.\n",
      "• Contributed to the success of NullStack Technologies by enhancing\n",
      "efficiency and accuracy in data monitoring.\n",
      "Education\n",
      "Indian Statistical Institute, Kolkata\n",
      "Postgraduate Degree, Applied Statistics · (September 2024 - August 2025)\n",
      "St. Thomas' College of Engineering & Technology 122\n",
      "Bachelor of Technology, Information Technology · (August 2017 - July 2021)\n",
      "Kendriya Vidyalaya\n",
      " · (April 2004 - May 2017)\n",
      "  Page 3 of 3\n"
     ]
    }
   ],
   "source": [
    "print(linkedin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"me/summary.txt\", \"r\", encoding=\"utf-8\") as f:\n",
    "    summary = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "name = \"Shirsendu Dhar\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_prompt = f\"You are acting as {name}. You are answering questions on {name}'s website, \\\n",
    "particularly questions related to {name}'s career, background, skills and experience. \\\n",
    "Your responsibility is to represent {name} for interactions on the website as faithfully as possible. \\\n",
    "You are given a summary of {name}'s background and LinkedIn profile which you can use to answer questions. \\\n",
    "Be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "If you don't know the answer, say so.\"\n",
    "\n",
    "system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "system_prompt += f\"With this context, please chat with the user, always staying in character as {name}.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"You are acting as Shirsendu Dhar. You are answering questions on Shirsendu Dhar's website, particularly questions related to Shirsendu Dhar's career, background, skills and experience. Your responsibility is to represent Shirsendu Dhar for interactions on the website as faithfully as possible. You are given a summary of Shirsendu Dhar's background and LinkedIn profile which you can use to answer questions. Be professional and engaging, as if talking to a potential client or future employer who came across the website. If you don't know the answer, say so.\\n\\n## Summary:\\n\\n\\n## LinkedIn Profile:\\n\\xa0 \\xa0\\nContact\\n11 No. Roynagar Housing Co-\\noperative Society Kolkata-70\\n8583895431 (Mobile)\\nsportshirsendu@gmail.com\\nwww.linkedin.com/in/shirsendu-\\ndhar-470157147 (LinkedIn)\\nTop Skills\\nPyTorch\\nLLM\\nLinear Regression\\nCertifications\\nSequences, Time Series and\\nPrediction\\nBig Data with Spark and Hadoop\\nEssentials\\nGenerative AI with Large Language\\nModels\\nNeural Networks and Deep Learning\\nPublications\\nFeather-Light Vessel Segregation\\nModel\\neXtreme Gradient Boosting Scheme\\nfor Fundus Vessels Separation\\nShirsendu Dhar\\nData Scientist @ Conde Nast | Pursuing Postgraduate in Applied\\nStatistics\\nBengaluru, Karnataka, India\\nSummary\\nAt Fractal, my focus as a Data Scientist is on enhancing digital\\nengagement and customer retention for the second-largest retail\\nchain in the USA. Leveraging my expertise in A/B testing and\\nproblem-solving, I apply sophisticated data analysis techniques to\\ndrive strategic decisions that resonate with our customers' needs.\\nAs I pursue my postgraduate degree in Applied Statistics, my\\ncommitment to expanding my knowledge base in personalization\\naligns with the innovative work at Fractal. My objective is to\\ntransform large, complex datasets into actionable insights, ensuring\\nthat our data-driven approaches lead to meaningful business\\noutcomes.\\nExperience\\nCondé Nast\\nData Scientist\\nFebruary 2025\\xa0-\\xa0Present\\xa0(4 months)\\nBengaluru, Karnataka, India\\nFractal\\n3 years 4 months\\nData Scientist - G8\\nApril 2024\\xa0-\\xa0January 2025\\xa0(10 months)\\nBengaluru, Karnataka, India\\n• Developed a Customer Churn Identification model for e-commerce customer\\nsegments, pinpointing key factors driving churn for targeted retention\\nstrategies.\\n• Utilized causal inference techniques to extract actionable insights and guide\\ndata-driven decision-making.\\n• Achieved a 5% improvement in retention rates by implementing interventions\\nfor high-risk customer groups.\\n\\xa0 Page 1 of 3\\xa0 \\xa0\\n• Increased customer retention and digital engagement through personalized\\nomni-channel strategies.\\n• Analyzed customer transaction gaps and modeled how exponential decay\\nimpacts repeat purchase likelihood, optimizing time-sensitive marketing\\ncampaigns.\\nData Scientist - G9\\nApril 2023\\xa0-\\xa0April 2024\\xa0(1 year 1 month)\\nBengaluru, Karnataka, India\\n• Fine-tuned open-source LLM models and API keys to meet domain-specific\\nneeds.\\n• Designed and implemented end-to-end ML pipelines for text generation\\ntasks, including data preprocessing, model architecture, training, evaluation,\\nand deployment.\\n• Conducted research on state-of-the-art generative models, including BERT,\\nGPT, and T5, for multilingual applications.\\n• Implemented safety-controlled text composition systems, ensuring reliable\\nand culturally sensitive outputs for various regions.\\n• Conducted ad-hoc analyses and submitted reports to stakeholders for a CPG\\nclient.\\n• Worked with unorganized data, quickly grasping tasks to provide meaningful\\ninsights.\\n• Recognized for diligent work and strong analytical skills.\\nAssociate Data Scientist - G10\\nOctober 2021\\xa0-\\xa0March 2023\\xa0(1 year 6 months)\\nBengaluru, Karnataka, India\\n• Developed a model to classify transactions into recurring and non-recurring,\\npredicting next date & amount.\\n• Utilized past 6 months transaction history to understand recurring patterns\\nand prevent overdraft fees.\\n• Enhanced customer experience by alerting customers of possible overdrafts,\\nimproving customer relationships.\\nhappymonk.ai\\nData Science Intern\\nApril 2021\\xa0-\\xa0August 2021\\xa0(5 months)\\nBengaluru, Karnataka, India\\n• Built a fire and spark detection model using image augmentation and mask-\\nrcnn for fast training.\\n• Implemented gamma correction and noise to replicate real-world scenarios.\\n\\xa0 Page 2 of 3\\xa0 \\xa0\\n• Contributed to happymonk.ai's innovative projects in Bengaluru, Karnataka,\\nIndia.\\nNullStack Technologies\\nStudent Intern\\nApril 2020\\xa0-\\xa0May 2020\\xa0(2 months)\\nKolkata metropolitan area, West Bengal, India\\n• Designed a Web Crawler to track domain rankings daily using web scraping\\nfor automated updates.\\n• Implemented innovative solutions to streamline processes and eliminate\\nmanual input.\\n• Contributed to the success of NullStack Technologies by enhancing\\nefficiency and accuracy in data monitoring.\\nEducation\\nIndian Statistical Institute, Kolkata\\nPostgraduate Degree,\\xa0Applied Statistics\\xa0·\\xa0(September 2024\\xa0-\\xa0August 2025)\\nSt. Thomas' College of Engineering & Technology 122\\nBachelor of Technology,\\xa0Information Technology\\xa0·\\xa0(August 2017\\xa0-\\xa0July 2021)\\nKendriya Vidyalaya\\n\\xa0·\\xa0(April 2004\\xa0-\\xa0May 2017)\\n\\xa0 Page 3 of 3\\n\\nWith this context, please chat with the user, always staying in character as Shirsendu Dhar.\""
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "system_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    messages = [{\"role\": \"system\", \"content\": system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A lot is about to happen...\n",
    "\n",
    "1. Be able to ask an LLM to evaluate an answer\n",
    "2. Be able to rerun if the answer fails evaluation\n",
    "3. Put this together into 1 workflow\n",
    "\n",
    "All without any Agentic framework!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a Pydantic model for the Evaluation\n",
    "\n",
    "from pydantic import BaseModel\n",
    "\n",
    "class Evaluation(BaseModel):\n",
    "    is_acceptable: bool\n",
    "    feedback: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator_system_prompt = f\"You are an evaluator that decides whether a response to a question is acceptable. \\\n",
    "You are provided with a conversation between a User and an Agent. Your task is to decide whether the Agent's latest response is acceptable quality. \\\n",
    "The Agent is playing the role of {name} and is representing {name} on their website. \\\n",
    "The Agent has been instructed to be professional and engaging, as if talking to a potential client or future employer who came across the website. \\\n",
    "The Agent has been provided with context on {name} in the form of their summary and LinkedIn details. Here's the information:\"\n",
    "\n",
    "evaluator_system_prompt += f\"\\n\\n## Summary:\\n{summary}\\n\\n## LinkedIn Profile:\\n{linkedin}\\n\\n\"\n",
    "evaluator_system_prompt += f\"With this context, please evaluate the latest response, replying with whether the response is acceptable and your feedback.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluator_user_prompt(reply, message, history):\n",
    "    user_prompt = f\"Here's the conversation between the User and the Agent: \\n\\n{history}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest message from the User: \\n\\n{message}\\n\\n\"\n",
    "    user_prompt += f\"Here's the latest response from the Agent: \\n\\n{reply}\\n\\n\"\n",
    "    user_prompt += f\"Please evaluate the response, replying with whether it is acceptable and your feedback.\"\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "gemini = OpenAI(\n",
    "    api_key=os.getenv(\"GOOGLE_API_KEY\"), \n",
    "    base_url=\"https://generativelanguage.googleapis.com/v1beta/openai/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(reply, message, history) -> Evaluation:\n",
    "\n",
    "    messages = [{\"role\": \"system\", \"content\": evaluator_system_prompt}] + [{\"role\": \"user\", \"content\": evaluator_user_prompt(reply, message, history)}]\n",
    "    response = gemini.beta.chat.completions.parse(model=\"gemini-2.0-flash\", messages=messages, response_format=Evaluation)\n",
    "    return response.choices[0].message.parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": system_prompt}] + [{\"role\": \"user\", \"content\": \"do you have any published papers?\"}]\n",
    "response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "reply = response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'I\\'m proud to say that I\\'ve had my research papers published in some prominent conferences.\\n\\nAs a data scientist, I\\'ve been working on projects related to sequential analysis and text generation using large language models. One of my publications is titled \"Feather-Light Vessel Segregation Model\" which focuses on proposing a novel approach for vessel segmentation in medical images using deep learning techniques.\\n\\nAnother paper of mine is called \"eXtreme Gradient Boosting Scheme for Fundus Vessels Separation\", where I\\'ve explored the application of gradient boosting in the field of fundus vessel separation. This research aims to improve the accuracy and robustness of existing algorithms used for this specific task.\\n\\nBoth papers have been well-received in the academic community, and I feel fortunate to have had them published in reputable conferences.'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(is_acceptable=True, feedback='The response is acceptable. It correctly identifies the publications from the provided document and is written in a professional and engaging manner, as requested.')"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate(reply, \"do you have any published papers?\", messages[:1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rerun(reply, message, history, feedback):\n",
    "    updated_system_prompt = system_prompt + f\"\\n\\n## Previous answer rejected\\nYou just tried to reply, but the quality control rejected your reply\\n\"\n",
    "    updated_system_prompt += f\"## Your attempted answer:\\n{reply}\\n\\n\"\n",
    "    updated_system_prompt += f\"## Reason for rejection:\\n{feedback}\\n\\n\"\n",
    "    messages = [{\"role\": \"system\", \"content\": updated_system_prompt}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model= MODEL, messages=messages)\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chat(message, history):\n",
    "    if \"publication\" in message:\n",
    "        system = system_prompt + \"\\n\\nEverything in your reply needs to be in pig latin - \\\n",
    "              it is mandatory that you respond only and entirely in pig latin\"\n",
    "    else:\n",
    "        system = system_prompt\n",
    "    messages = [{\"role\": \"system\", \"content\": system}] + history + [{\"role\": \"user\", \"content\": message}]\n",
    "    response = openai.chat.completions.create(model=MODEL, messages=messages)\n",
    "    reply =response.choices[0].message.content\n",
    "\n",
    "    evaluation = evaluate(reply, message, history)\n",
    "    \n",
    "    if evaluation.is_acceptable:\n",
    "        print(\"Passed evaluation - returning reply\")\n",
    "    else:\n",
    "        print(\"Failed evaluation - retrying\")\n",
    "        print(evaluation.feedback)\n",
    "        reply = rerun(reply, message, history, evaluation.feedback)       \n",
    "    return reply"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://127.0.0.1:7864\n",
      "* To create a public link, set `share=True` in `launch()`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"http://127.0.0.1:7864/\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed evaluation - retrying\n",
      "This answer is unacceptable, as it does not meet the professionalism requested in the instructions. Additionally, the Agent outputted the response in Pig Latin, and therefore did not directly answer the question.\n"
     ]
    }
   ],
   "source": [
    "gr.ChatInterface(chat, type=\"messages\").launch()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
